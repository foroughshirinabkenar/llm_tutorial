{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "192e6ea4-d637-4984-81a5-9aec39cc324e",
   "metadata": {},
   "source": [
    "# mini-Transformer (from scratch)\n",
    "$\\textbf{Goal:}$ implement a tiny encoder-only Transformer and train it on character data (no external libs beyond PyTorch). You will learn tokenization (chars), attention, causal masks, training loop, sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37527227-c93b-401d-a585-92b62cabe2f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11470cf10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DEVICE = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "BLOCK_SIZE = 8\n",
    "BATCH_SIZE = 32\n",
    "LR         = 3e-4\n",
    "MLM_PROB   = 0.1\n",
    "SEED       = 42\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9641b4b9-ed39-46cd-a92b-4df8cd506e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Rumi was born to Persian parents in Balkh modern-day Afghanistan or Wakhsh a village \n",
    "on the East bank of the Wakhsh River known as Sangtuda in present-day Tajikistan.\n",
    "The area, culturally adjacent to Balkh, is where Mawlânâ's father, Bahâ' uddîn Walad, \n",
    "was a preacher and jurist. He lived and worked there until 1212, when Rumi was aged \n",
    "around five and the family moved to Samarkand. Greater Balkh was at that time a major \n",
    "centre of Persian culture and Sufism had developed there for several centuries. The \n",
    "most important influences upon Rumi, besides his father, were the Persian poets Attar \n",
    "and Sanai. Rumi expresses his appreciation: \"Attar was the spirit, Sanai his eyes twain, \n",
    "And in time thereafter, Came we in their train\" and mentions in another poem: \"Attar has \n",
    "traversed the seven cities of Love, We are still at the turn of one street\". His father \n",
    "was also connected to the spiritual lineage of Najm al-Din Kubra. Rumi lived most of his \n",
    "life under the Persianate Seljuk Sultanate of Rum, where he produced his works and died \n",
    "in 1273 AD. He was buried in Konya, and his shrine became a place of pilgrimage. Upon his \n",
    "death, his followers and his son Sultan Walad founded the Mevlevi Order, also known as \n",
    "the Order of the Whirling Dervishes, famous for the Sufi dance known as the Sama ceremony. \n",
    "He was laid to rest beside his father, and over his remains a shrine was erected. A hagiographical \n",
    "account of him is described in Shams ud-Din Ahmad Aflāki's Manāqib ul-Ārifīn (written between 1318 \n",
    "and 1353). This biography needs to be treated with care as it contains both legends and facts about \n",
    "Rumi. For example, Professor Franklin Lewis of the University of Chicago, author of the most complete \n",
    "biography on Rumi, has separate sections for the hagiographical biography of Rumi and the actual \n",
    "biography about him.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72f3762a-6dae-48f9-b3c1-5391826058bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dictionary and encoding function\n",
    "\n",
    "# remove newlines\n",
    "text = text.replace('\\n', ' ')\n",
    "\n",
    "# convert text to characters\n",
    "words = text.split()\n",
    "\n",
    "# size of vocabulary\n",
    "vocab = list(set(words))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# string to integer\n",
    "stoi = {c: i for i, c in enumerate(vocab)}\n",
    "\n",
    "stoi['[MASK]'] = len(stoi)\n",
    "mask_token_id = stoi['[MASK]']\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "# encode\n",
    "encode = lambda s: torch.tensor([stoi[c] for c in s.split()], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db13f880-6ea6-401a-968c-2484cb9cdeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_tokens(inputs, vocabsize, mask_token_id, mlm_prob=0.15):\n",
    "    labels = inputs.clone()    # create a copy of the inputs\n",
    "    probability_matrix = torch.full(labels.shape, mlm_prob)      # based on the masked language model (MLM)\n",
    "    mask_indices = torch.bernoulli(probability_matrix).bool()    # indices we aim to mask by mask_token, e.e, [MASK]\n",
    "\n",
    "    labels[~mask_indices] = -100                                 # ignore non-masked tokens in loss\n",
    "\n",
    "    # replacing (80/10/10)\n",
    "    # ~80% replace with mask_token\n",
    "    # ~10% replace with random token\n",
    "    # ~10% unchanged\n",
    "\n",
    "    # replace with mask_token\n",
    "    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & mask_indices\n",
    "    inputs[indices_replaced] = mask_token_id\n",
    "\n",
    "    # replace with random token\n",
    "    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & mask_indices & ~indices_replaced\n",
    "    random_tokens = torch.randint(vocabsize, size=(indices_random.sum().item(),), dtype=torch.long)\n",
    "    inputs[indices_random] = random_tokens\n",
    "\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7239b32c-6402-41ee-a72a-5b89cbe53c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = encode(text)\n",
    "def get_batch(mask_token_id, vocabsize, batch_size=32, block_size=8):\n",
    "    ix = torch.randint(0, len(data) - block_size - 1, (batch_size,))\n",
    "    x = torch.stack([data[i: i+block_size] for i in ix])             # (batch_size, block_size)\n",
    "    y = torch.stack([data[i+1: i+1+block_size] for i in ix])         # (batch_size, block_size)\n",
    "    x, y = mask_tokens(x.clone(), vocabsize, mask_token_id)\n",
    "\n",
    "    return x.to(DEVICE), y.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7802c22-b842-4d29-af02-2f8385f19f21",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1c82a1e-89bc-4788-912b-066dbeaebe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, n_embed, head_size, dropout):\n",
    "        super(Head, self).__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k, q, v = self.key(x), self.query(x), self.value(x)\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(k.size(-1))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "        out = att @ v\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6335f984-9975-449d-98f9-9e35a01adc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHead(nn.Module):\n",
    "    def __init__(self, n_embed, n_head, dropout):\n",
    "        super(MultiHead, self).__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.heads = nn.ModuleList([Head(n_embed, head_size, dropout) for _ in range(n_head)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)   # each h: (B, T, head_size) -> concat: (B, T, n_head*head_size)\n",
    "        proj = self.proj(out)                                 # n_head*head_size = embed_dim -> project them to embed_dim\n",
    "        return self.dropout(proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55e45e26-c16f-419d-a436-70f765dd94b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed, dropout):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4*n_embed),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*n_embed, n_embed),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baa6ce1d-7500-49ed-b93d-df6043744fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_head, dropout):\n",
    "        super(Block, self).__init__()\n",
    "        self.mh = MultiHead(n_embed, n_head, dropout)\n",
    "        self.ff = FeedForward(n_embed, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_p = self.ln1(x)\n",
    "        x = x + self.mh(x_p)\n",
    "        x_p = self.ln2(x)\n",
    "        x = x + self.ff(x_p)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39dca985-9642-43fd-9e1b-fb11f1c35249",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyBERT(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, n_embed=128, n_head=4, n_layer=4, dropout=0.1):\n",
    "        super(TinyBERT, self).__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, n_embed)\n",
    "        self.pos_embed = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embed, n_head, dropout) for _ in range(n_layer)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.mlm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, labels=None):\n",
    "        B, T = idx.shape\n",
    "        token = self.token_embed(idx)\n",
    "        pos = self.pos_embed(torch.arange(T, device=idx.device))\n",
    "        x = token + pos\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.mlm_head(x)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=-100)\n",
    "            \n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90d9b60d-0614-459d-b3d5-13e584866354",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TinyBERT(vocab_size, BLOCK_SIZE).to(DEVICE)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cf5eaa-498f-485a-9789-d104005dc5ce",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "996ebe9d-c22b-40cd-b259-e5f744fa86b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000, Loss:  4.8240, Accuracy:  5.88\n",
      "Epoch 200/1000, Loss:  3.9187, Accuracy:  16.67\n",
      "Epoch 300/1000, Loss:  3.5331, Accuracy:  30.43\n",
      "Epoch 400/1000, Loss:  2.6440, Accuracy:  46.81\n",
      "Epoch 500/1000, Loss:  2.0034, Accuracy:  70.00\n",
      "Epoch 600/1000, Loss:  1.5215, Accuracy:  65.96\n",
      "Epoch 700/1000, Loss:  1.3033, Accuracy:  74.00\n",
      "Epoch 800/1000, Loss:  1.0863, Accuracy:  86.67\n",
      "Epoch 900/1000, Loss:  0.6710, Accuracy:  91.89\n",
      "Epoch 1000/1000, Loss:  0.6087, Accuracy:  95.83\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    x_batch, y_batch = get_batch(mask_token_id, vocab_size, batch_size=BATCH_SIZE, block_size=BLOCK_SIZE)\n",
    "    logits, loss = model(x_batch, y_batch)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    mask = y_batch != -100\n",
    "    preds = torch.argmax(logits, dim=-1)\n",
    "    correct = (preds[mask] == y_batch[mask]).sum().item()\n",
    "    acc = correct / mask.sum().item()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item(): .4f}, Accuracy: {acc * 100: .2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff5c35e-5607-4bcc-ba7c-956e928fbda8",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07888b2b-03cf-47b6-b6fa-1b6d280e302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def evaluate_model(model, data, orig_data=None, mask_flag=True, block_size=32, batch_size=32, mask_token_id=None, vocab_size=None):\n",
    "    \"\"\"\n",
    "    Evaluates TinyBERT (MLM) on a dataset and computes:\n",
    "    - Cross-Entropy Loss\n",
    "    - Perplexity (PPL)\n",
    "    - Masked Token Accuracy\n",
    "    - Bits Per Character (optional)\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data) - block_size - 1, batch_size):\n",
    "            x = torch.stack([\n",
    "                data[j:j+block_size] \n",
    "                for j in range(i, min(i+batch_size, len(data)-block_size-1))\n",
    "            ]).clone()\n",
    "\n",
    "            # mask tokens\n",
    "            if mask_flag:\n",
    "                x_true = x.clone()\n",
    "                inputs, labels = mask_tokens(x, vocab_size, mask_token_id)\n",
    "            else:\n",
    "                x_true = torch.stack([\n",
    "                    orig_data[j:j+block_size] \n",
    "                    for j in range(i, min(i+batch_size, len(data)-block_size-1))\n",
    "                ]).clone()\n",
    "                inputs = x.clone()\n",
    "                labels = x.clone()\n",
    "                labels[labels != mask_token_id] = -100\n",
    "                \n",
    "            mask = labels != -100\n",
    "            if mask.sum().item() == 0:\n",
    "                continue\n",
    "                \n",
    "            x_true, inputs, labels = x_true.to(DEVICE), inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            logits, loss = model(inputs, x_true)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # compute accuracy only on masked tokens\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            correct += (preds[mask] == x_true[mask]).sum().item()\n",
    "            total += mask.sum().item()\n",
    "\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return {\n",
    "        \"CrossEntropyLoss\": avg_loss,\n",
    "        \"Perplexity\": perplexity,\n",
    "        \"MaskedAccuracy\": accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6332271c-2372-4d08-9726-ccf396f6bfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation Results ===\n",
      "Cross-Entropy Loss: 0.2245\n",
      "Perplexity (PPL)  : 1.25\n",
      "Masked Accuracy   : 100.00%\n"
     ]
    }
   ],
   "source": [
    "orig_text = \"\"\"\n",
    "Rumi was born to Persian parents in Balkh modern-day Afghanistan or Wakhsh a village \n",
    "on the East bank of the Wakhsh River known as Sangtuda in present-day Tajikistan.\n",
    "\"\"\"\n",
    "orig_data = encode(orig_text)\n",
    "\n",
    "eval_text = \"\"\"\n",
    "Rumi was born to [MASK] parents in Balkh modern-day Afghanistan or Wakhsh a village \n",
    "on the East bank of the [MASK] River known as Sangtuda in present-day Tajikistan.\n",
    "\"\"\"\n",
    "\n",
    "encode_eval = lambda s: torch.tensor([stoi.get(w, mask_token_id) for w in s.split()], dtype=torch.long)\n",
    "eval_data = encode_eval(eval_text)\n",
    "\n",
    "metrics = evaluate_model(model,\n",
    "                         eval_data,\n",
    "                         orig_data,\n",
    "                         mask_flag=False,\n",
    "                         block_size=BLOCK_SIZE, \n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         mask_token_id=mask_token_id,\n",
    "                         vocab_size=vocab_size)\n",
    "\n",
    "print(\"\\n=== Evaluation Results ===\")\n",
    "print(f\"Cross-Entropy Loss: {metrics['CrossEntropyLoss']:.4f}\")\n",
    "print(f\"Perplexity (PPL)  : {metrics['Perplexity']:.2f}\")\n",
    "print(f\"Masked Accuracy   : {metrics['MaskedAccuracy']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f26c023-c690-41f6-b359-d27196cb4dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
