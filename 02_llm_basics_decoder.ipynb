{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "192e6ea4-d637-4984-81a5-9aec39cc324e",
   "metadata": {},
   "source": [
    "# mini-Transformer (from scratch)\n",
    "$\\textbf{Goal:}$ implement a tiny decoder-only Transformer and train it on character data (no external libs beyond PyTorch). You will learn tokenization (chars), attention, causal masks, training loop, sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37527227-c93b-401d-a585-92b62cabe2f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x111a0ceb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "BLOCK_SIZE = 64\n",
    "BATCH_SIZE = 64\n",
    "LR         = 3e-4\n",
    "SEED       = 42\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9641b4b9-ed39-46cd-a92b-4df8cd506e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Jalāl al-Dīn Muḥammad Rūmī, or simply Rumi, was a 13th-century poet, Hanafi \n",
    "faqih, Maturidi theologian, and Sufi mystic born during the Khwarazmian Empire. \n",
    "Rumi's works are written in his mother tongue, Persian. He occasionally used the \n",
    "Arabic language and single Turkish and Greek words in his verse.\"\"\"\n",
    "\n",
    "# convert text to characters\n",
    "chars = list(set(text))\n",
    "\n",
    "# string to integer\n",
    "stoi = {c: i for i, c in enumerate(chars)}\n",
    "\n",
    "# integer to string\n",
    "itos = {i: c for c, i in stoi.items()}\n",
    "\n",
    "# encode\n",
    "encode = lambda s: torch.tensor([stoi[c] for c in s], dtype=torch.long)\n",
    "\n",
    "# decode\n",
    "decode = lambda t: ''.join(itos[int(i)] for i in t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7239b32c-6402-41ee-a72a-5b89cbe53c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = encode(text)\n",
    "def get_batch(block_size=16, batch_size=32):\n",
    "    ix = torch.randint(0, len(data) - block_size - 1, (batch_size,))\n",
    "    x = torch.stack([data[i: i+block_size] for i in ix])             # (batch_size, block_size)\n",
    "    y = torch.stack([data[i+1: i+1+block_size] for i in ix])         # (batch_size, block_size)\n",
    "\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7802c22-b842-4d29-af02-2f8385f19f21",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1c82a1e-89bc-4788-912b-066dbeaebe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, n_embed, head_size, block_size, dropout):\n",
    "        super(Head, self).__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size))) # lower triangle (masking)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k, q, v = self.key(x), self.query(x), self.value(x)\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(k.size(-1))\n",
    "        att = att.masked_fill(self.tril[:T, :T]==0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "        out = att @ v\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6335f984-9975-449d-98f9-9e35a01adc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHead(nn.Module):\n",
    "    def __init__(self, n_embed, n_head, block_size, dropout):\n",
    "        super(MultiHead, self).__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.heads = nn.ModuleList([Head(n_embed, head_size, block_size, dropout) for _ in range(n_head)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)   # each h: (B, T, head_size) -> concat: (B, T, n_head*head_size)\n",
    "        proj = self.proj(out)                                 # n_head*head_size = embed_dim -> project them to embed_dim\n",
    "        return self.dropout(proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55e45e26-c16f-419d-a436-70f765dd94b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed, dropout):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4*n_embed),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*n_embed, n_embed),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baa6ce1d-7500-49ed-b93d-df6043744fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_head, block_size, dropout):\n",
    "        super(Block, self).__init__()\n",
    "        self.mh = MultiHead(n_embed, n_head, block_size, dropout)\n",
    "        self.ff = FeedForward(n_embed, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_p = self.ln1(x)\n",
    "        x = x + self.mh(x_p)\n",
    "        x_p = self.ln2(x)\n",
    "        x = x + self.ff(x_p)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39dca985-9642-43fd-9e1b-fb11f1c35249",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, n_embed=128, n_head=4, n_layer=4, dropout=0.1):\n",
    "        super(TinyGPT, self).__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, n_embed)\n",
    "        self.pos_embed = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embed, n_head, block_size, dropout) for _ in range(n_layer)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        token = self.token_embed(idx)\n",
    "        pos = self.pos_embed(torch.arange(T, device=idx.device))\n",
    "        x = token + pos\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            \n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90d9b60d-0614-459d-b3d5-13e584866354",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(chars)\n",
    "model = TinyGPT(vocab_size, BLOCK_SIZE).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cf5eaa-498f-485a-9789-d104005dc5ce",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "996ebe9d-c22b-40cd-b259-e5f744fa86b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000, Loss:  1.6484\n",
      "Epoch 200/1000, Loss:  0.4358\n",
      "Epoch 300/1000, Loss:  0.1481\n",
      "Epoch 400/1000, Loss:  0.0896\n",
      "Epoch 500/1000, Loss:  0.0778\n",
      "Epoch 600/1000, Loss:  0.0650\n",
      "Epoch 700/1000, Loss:  0.0572\n",
      "Epoch 800/1000, Loss:  0.0520\n",
      "Epoch 900/1000, Loss:  0.0503\n",
      "Epoch 1000/1000, Loss:  0.0469\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    x_batch, y_batch = get_batch(block_size=BLOCK_SIZE, batch_size=BATCH_SIZE)\n",
    "    _, loss = model(x_batch, y_batch)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item(): .4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905952dd-e1b9-4982-ade3-c1889f6e359a",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c19887c5-6b8f-41ee-93fc-7fbd7f712227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jalāl al-Dīn Muḥammad Rūmī, or simply Rumi, was a 13th-century poet, Hanafi \n",
      "faqih, Maturidi theologian, and Sufi mystic born during the Khwarazmian Empire. \n",
      "Rumi's works are written in his mother tongue, Persian. He occasionally used the \n",
      "Arabic language and single Turkish and Greek words in his verserserdsiothersiangue, Persian. He occasionally used the \n",
      "Arabic language and single Turkish and Greek wo\n"
     ]
    }
   ],
   "source": [
    "def generate(prompt=\"Jalāl \", block_size=16, max_new_tokens=400, temperature=0.8, top_k=50):\n",
    "    idx = encode(prompt).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = model(idx[:, -block_size:])\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = float('-inf')\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, next_id], dim=1)\n",
    "            \n",
    "    return decode(idx[0].cpu())\n",
    "    \n",
    "print(generate(block_size=BLOCK_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff5c35e-5607-4bcc-ba7c-956e928fbda8",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44e6e4de-ae2c-43f3-826a-37d86aa7b44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct_n_score(text, n=1):\n",
    "    tokens = list(text)\n",
    "    if len(tokens) < n:\n",
    "        return 0.0\n",
    "    ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
    "    \n",
    "    return len(set(ngrams)) / len(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07888b2b-03cf-47b6-b6fa-1b6d280e302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def evaluate_model(model, data, block_size=8, batch_size=8):\n",
    "    \"\"\"\n",
    "    Evaluates TinyGPT on a dataset and computes several metrics:\n",
    "    - Cross-Entropy Loss\n",
    "    - Perplexity (PPL)\n",
    "    - Bits Per Character (BPC)\n",
    "    - Next-character Accuracy\n",
    "    - Distinct-1 / Distinct-2 (diversity of generated text)\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Break data into batches for evaluation\n",
    "    with torch.no_grad():\n",
    "        # x_batch, y_batch = get_batch(batch_size=batch_size)\n",
    "        # for x, y in zip(x_batch, y_batch):\n",
    "        for i in range(0, len(data) - block_size - 1, batch_size):\n",
    "            x = torch.stack([data[j:j+block_size] for j in range(i, min(i+batch_size, len(data) - block_size - 1))])\n",
    "            y = torch.stack([data[j+1:j+1+block_size] for j in range(i, min(i+batch_size, len(data) - block_size - 1))])\n",
    "\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits, loss = model(x, y)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # accuracy (next-character prediction)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.numel()\n",
    "\n",
    "    # average cross-entropy loss\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    \n",
    "    # perplexity\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    \n",
    "    # bits per character (BPC)\n",
    "    bpc = avg_loss / math.log(2)\n",
    "    \n",
    "    # next-character prediction accuracy\n",
    "    accuracy = correct / total\n",
    "\n",
    "    # generate text for diversity metrics\n",
    "    generated_text = generate(prompt=\"Jalāl \", max_new_tokens=400)\n",
    "    distinct1 = distinct_n_score(generated_text, 1)\n",
    "    distinct2 = distinct_n_score(generated_text, 2)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return {\n",
    "        \"CrossEntropyLoss\": avg_loss,\n",
    "        \"Perplexity\": perplexity,\n",
    "        \"BitsPerCharacter\": bpc,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Distinct-1\": distinct1,\n",
    "        \"Distinct-2\": distinct2,\n",
    "        \"GeneratedSample\": generated_text\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6332271c-2372-4d08-9726-ccf396f6bfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation Results ===\n",
      "Cross-Entropy Loss: 0.0399\n",
      "Perplexity (PPL):   1.04\n",
      "Bits Per Char:      0.058 bits\n",
      "Accuracy:           98.48%\n",
      "Distinct-1:         0.113\n",
      "Distinct-2:         0.432\n",
      "\n",
      "=== Generated Sample ===\n",
      "\n",
      "Jalāl al-Dīn Muḥammad Rūmī, or simply Rumi, was a 13th-century poet, Hanafi \n",
      "faqih, Maturidi theologian, and Sufi mystic born during the Khwarazmian Empire. \n",
      "Rumi's works are written in his mother tongue, Persian. He occasionally used the \n",
      "Arabic language and single Turksh are waritten in his mother tongue, Persian. He occasionally used the \n",
      "Arabic language and Sufi mystic born during the Khwarazmian Em\n"
     ]
    }
   ],
   "source": [
    "eval_data = encode(text)\n",
    "\n",
    "metrics = evaluate_model(model, eval_data, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(\"\\n=== Evaluation Results ===\")\n",
    "print(f\"Cross-Entropy Loss: {metrics['CrossEntropyLoss']:.4f}\")\n",
    "print(f\"Perplexity (PPL):   {metrics['Perplexity']:.2f}\")\n",
    "print(f\"Bits Per Char:      {metrics['BitsPerCharacter']:.3f} bits\")\n",
    "print(f\"Accuracy:           {metrics['Accuracy']*100:.2f}%\")\n",
    "print(f\"Distinct-1:         {metrics['Distinct-1']:.3f}\")\n",
    "print(f\"Distinct-2:         {metrics['Distinct-2']:.3f}\")\n",
    "print(\"\\n=== Generated Sample ===\\n\")\n",
    "print(metrics[\"GeneratedSample\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40647dd6-0065-4ada-8ef8-10c538d7c728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
