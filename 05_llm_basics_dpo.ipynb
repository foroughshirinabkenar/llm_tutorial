{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20e8b318-2294-4e11-b205-f741b90b10e1",
   "metadata": {},
   "source": [
    "# Direct Preference Optimization (DPO)\n",
    "In this project, we fine-tune the <b>GPT-2</b> model using DPO. In this regard, we import the required models and libraries from <a href=\"https://huggingface.co/\" target=\"_blank\"><strong>Hugging Face</strong></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3832270b-17ee-4712-82ad-5f0d0e693dbe",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8778dbcf-de00-4877-9da5-02a504f70438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "from transformers import AutoTokenizer           # loads tokenizer for the chosen model\n",
    "from transformers import AutoModelForCausalLM    # loads a causal language model\n",
    "from trl import DPOTrainer                       # loads trainer for DPO\n",
    "from trl import DPOConfig                        # defines DPO hyperparameters\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d977da63-78ef-4e7c-88cb-618cdc35d3f9",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d5cd42c-9707-48c3-ac68-4a969c642909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'response', 'chosen', 'rejected'],\n",
       "        num_rows: 76256\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'response', 'chosen', 'rejected'],\n",
       "        num_rows: 5103\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_dataset = load_dataset(\"Dahoas/rm-static\")\n",
    "orig_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c17c32a-b392-474c-8fc5-94449487d827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'response', 'chosen', 'rejected'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'response', 'chosen', 'rejected'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train = 2000\n",
    "n_eval  = 500\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": orig_dataset[\"train\"].select(range(n_train)),\n",
    "    \"test\": orig_dataset[\"test\"].select(range(n_eval))\n",
    "})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714f3d6f-a3ab-49cd-81fe-a52e36ef278e",
   "metadata": {},
   "source": [
    "## Define Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23804bab-a0f5-41a3-809d-60fd60d2e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044027e2-868a-4ed9-8835-d12b1af6a2ef",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f939b9df-485e-491c-9c3c-662d32a04d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# some models do not own a dedicated padding token; thus, we set it manually\n",
    "# using end-of-sequence (eos) token to avoid errors\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e7e490-b003-4687-95f8-b65134712ebc",
   "metadata": {},
   "source": [
    "## Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea915de7-3701-4ea2-b7bc-d4c790807256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11725e0b175d43aaa7b4b7ab7ac852a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "323141fed2754371aafdac76c567310e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenization_fn(example):\n",
    "    out = tokenizer(\n",
    "        example[\"prompt\"] + example[\"chosen\"],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    return out\n",
    "\n",
    "tokenized_dataset = dataset[\"train\"].map(tokenization_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73fc5b5-d3b9-4681-85a1-6d9fcc8aca4a",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaec3da6-8ed1-4f0e-98ec-d09a2c00908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, device_map=\"auto\", torch_dtype=\"auto\"\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754e0bc5-f4ef-4724-ad2d-6be37b892c94",
   "metadata": {},
   "source": [
    "## Define Training Hyperparameters\n",
    "We have a list of parameters and hyperparameters to set:\n",
    "<ul>\n",
    "    <li>\n",
    "        <b>output_dir:</b> the directory where checkpoints are saved.\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>per_device_train_batch_size:</b> keeps virtual random access memeory (VRAM) usage low.\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>learning_rate:</b> learning rate.\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>logging_steps:</b> number of steps for logging loss.\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>betal:</b> preference sharpness; higher value, stronger preference.\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>max_length:</b> maximum length of tokens.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c39b607-b633-42b6-bab4-e59c4d748797",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = DPOConfig(\n",
    "    output_dir='output_dir/dpo',\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-6,\n",
    "    beta=0.1,\n",
    "    max_length=512,\n",
    "    fp16=False,          # disables half precision (if it is not CUDA)\n",
    "    bf16=False,          # disables bfloat16 (if it is not CUDA)\n",
    "    no_cuda=True,\n",
    "    use_mps_device=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c390cb-befb-4b41-b24a-596133822e62",
   "metadata": {},
   "source": [
    "## Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afc2d1c4-745b-4dfa-b760-1dbfb3636df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "172cbdcf816747be8c891a75d3f50563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in train dataset:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad5c37e0f3b47d49471b82c9a725eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81efbbdaf38245cdadedb1fac4b38272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 26:01, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.684600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.672000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.668500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trianing time:  1618.5788\n"
     ]
    }
   ],
   "source": [
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=None,                            # will clone base as frozen reference internally\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    processing_class=tokenizer\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "train_time = time.time() - start_time\n",
    "print(f\"Trianing time: {train_time: .4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2a0822-159e-469f-9a8c-91573b152f82",
   "metadata": {},
   "source": [
    "## Save Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa31fc5d-d5cb-4505-813f-6348d711f319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"models/fine_tuning/dpo\")\n",
    "# tokenizer.save_pretrained(\"models/fine_tuning/dpo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4527f0f7-c4fa-451a-a084-19869819b872",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19142199-a64b-4699-ae93-63c41cc9b490",
   "metadata": {},
   "source": [
    "### Preference Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "793c486c-8417-4e2c-875f-35ce44d9f44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def log_prob(text, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    full_input = tokenizer(prompt + text, return_tensors=\"pt\").to(model.device)\n",
    "    labels = full_input[\"input_ids\"].clone()\n",
    "    labels[:, :len(inputs[\"input_ids\"][0])] = -100  # mask prompt tokens\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**full_input, labels=labels)\n",
    "        \n",
    "    return -outputs.loss.item() * len(labels[0])  # sum of log-probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b3f84f8-f2dc-4405-a819-2c4a8e4d9c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preference Accuracy = 0.456\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = dataset[\"test\"]\n",
    "\n",
    "correct = 0\n",
    "for example in eval_dataset:\n",
    "    lp_chosen = log_prob(example[\"chosen\"], example[\"prompt\"])\n",
    "    lp_rejected = log_prob(example[\"rejected\"], example[\"prompt\"])\n",
    "    if lp_chosen > lp_rejected:\n",
    "        correct += 1\n",
    "\n",
    "pref_acc = correct / len(eval_dataset)\n",
    "print(f\"Preference Accuracy = {pref_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5988d734-f2e5-44a6-abb0-dac689c24216",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
